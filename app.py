import gradio as gr
from rag_assistant import ClassicalJapaneseAssistant
from vector_store import JapaneseVectorStore
from ocr_pipeline import JapaneseOCR
from database_manager import DatabaseManager
from theme import create_japanese_theme, CUSTOM_CSS, SEASONAL_THEMES, get_seasonal_css
from ui_components import *
import os
import glob
import threading
import logging
import uuid
import time
from config import settings

# Initialize components
vector_store = JapaneseVectorStore()
assistant = ClassicalJapaneseAssistant(vector_store)
ocr = JapaneseOCR()
db_manager = DatabaseManager()

# Theme configuration
current_theme = create_japanese_theme()
current_season = "sakura"

# Session management
session_stop_events = {}
session_last_used = {}
last_sources = []
dictionary_entries = []  # Loaded dictionary entries for lookup

def _cleanup_expired_sessions():
    """Remove session events that haven't been used for a while."""
    ttl = max(1, int(getattr(settings, 'session_ttl_minutes', 60))) * 60
    now = time.time()
    expired = [sid for sid, ts in session_last_used.items() if now - ts > ttl]
    for sid in expired:
        session_stop_events.pop(sid, None)
        session_last_used.pop(sid, None)

def get_available_prompts():
    """Dynamically load all .md files from prompts folder"""
    prompt_files = glob.glob("prompts/*.md")
    return sorted(prompt_files) if prompt_files else ["prompts/classical_japanese_tutor.md"]

def enhanced_chat_function(message, history, show_thinking_enabled=True, knowledge_mode="auto", session_id=None):
    """Enhanced chat interface with streaming support and knowledge source selection"""
    global last_sources
    
    if session_id is None:
        session_id = str(uuid.uuid4())
    
    _cleanup_expired_sessions()
    if session_id not in session_stop_events:
        session_stop_events[session_id] = threading.Event()
    
    stop_event = session_stop_events[session_id]
    stop_event.clear()
    session_last_used[session_id] = time.time()
    
    history = history or []
    history.append({"role": "user", "content": message})
    
    # Check if a model is selected
    if not assistant.model_name:
        history.append({
            "role": "assistant", 
            "content": "âŒ ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n\nâŒ No model selected. Please select a model in the Settings tab first."
        })
        yield (
            history,
            gr.update(value="", visible=False),
            gr.update(value=""),
            gr.update(visible=False),
            gr.update()
        )
        return
    
    try:
        model_info = ""
        thinking_text = ""
        answer_text = ""
        sources_text = ""
        is_thinking_model = False
        stream_start_ts = time.time()
        
        # Stream the response with enhanced formatting using hybrid system
        for chunk in assistant.query_hybrid_stream(message, knowledge_mode=knowledge_mode, stop_event=stop_event):
            session_last_used[session_id] = time.time()
            
            if stop_event.is_set():
                if thinking_text:
                    thinking_text += "\n\n*[ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Generation stopped by user]*"
                if answer_text:
                    answer_text += "\n\n*[ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Generation stopped by user]*"
                else:
                    answer_text = "*[ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Generation stopped by user]*"
                
                if is_thinking_model:
                    if len(history) > 1 and history[-1]["role"] == "assistant":
                        history[-1]["content"] = f"ğŸ“ å¿œç­”ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Response stopped - using {assistant.model_name}"
                    else:
                        history.append({
                            "role": "assistant", 
                            "content": f"ğŸ“ å¿œç­”ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Response stopped - using {assistant.model_name}"
                        })
                
                yield (
                    history,
                    gr.update(value="", visible=False),
                    gr.update(value=thinking_text),
                    gr.update(visible=bool(thinking_text) and show_thinking_enabled),
                    gr.update(visible=False)
                )
                return
            
            # Process different types of chunks
            if chunk.get('type') == 'model_info':
                is_thinking_model = chunk.get('is_thinking_model', False)
                has_sources = chunk.get('sources') and len(chunk.get('sources', [])) > 0
                route = chunk.get('route', 'AUTO')
                confidence = chunk.get('confidence', 0.0)
                
                # Enhanced model info with routing information
                route_emojis = {
                    'RAG': 'ğŸ“š',
                    'GENERAL': 'ğŸ§ ', 
                    'HYBRID': 'ğŸ”„',
                    'AUTO': 'ğŸ¤–'
                }
                
                route_descriptions = {
                    'RAG': 'æ•™ç§‘æ›¸ã®ã¿ â€¢ Textbook Only',
                    'GENERAL': 'ãƒ¢ãƒ‡ãƒ«çŸ¥è­˜ã®ã¿ â€¢ Model Knowledge Only',
                    'HYBRID': 'æ•™ç§‘æ›¸+ãƒ¢ãƒ‡ãƒ«çŸ¥è­˜ â€¢ Textbook + Model Knowledge',
                    'AUTO': 'è‡ªå‹•é¸æŠ â€¢ Auto-Selected'
                }
                
                route_emoji = route_emojis.get(route, 'ğŸ¤–')
                route_desc = route_descriptions.get(route, 'Unknown')
                
                model_info = f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: **{assistant.model_name}** {'(æ¨è«–ãƒ¢ãƒ‡ãƒ« â€¢ Reasoning Model)' if is_thinking_model else ''}\n"
                model_info += f"{route_emoji} **çŸ¥è­˜ã‚½ãƒ¼ã‚¹ â€¢ Knowledge Source:** {route_desc}"
                
                if confidence > 0:
                    model_info += f" (ä¿¡é ¼åº¦ â€¢ Confidence: {confidence:.1%})"
                
                if chunk.get('sources'):
                    last_sources = chunk['sources']
            
            elif chunk.get('type') == 'thinking' and chunk.get('token'):
                thinking_text += chunk['token']
                elapsed = time.time() - stream_start_ts
                metrics_line = f"â± {elapsed:.1f}s â€¢ æ€è€ƒ {len(thinking_text)} æ–‡å­—"
                yield (
                    history,
                    gr.update(value=model_info, visible=bool(model_info)),
                    gr.update(value=f"{metrics_line}\n\n" + thinking_text),
                    gr.update(visible=show_thinking_enabled and is_thinking_model),
                    gr.update(visible=True)
                )
            
            elif chunk.get('type') == 'answer' and chunk.get('token'):
                answer_text += chunk['token']
                elapsed = time.time() - stream_start_ts
                metrics_line = f"â± {elapsed:.1f}s â€¢ æ€è€ƒ {len(thinking_text)} æ–‡å­— â€¢ å¿œç­” {len(answer_text)} æ–‡å­—"
                
                if len(history) > 0 and history[-1]["role"] == "assistant":
                    history[-1]["content"] = answer_text
                else:
                    history.append({"role": "assistant", "content": answer_text})
                
                yield (
                    history,
                    gr.update(value=model_info, visible=bool(model_info)),
                    gr.update(value=f"{metrics_line}\n\n" + thinking_text),
                    gr.update(visible=show_thinking_enabled and is_thinking_model and bool(thinking_text)),
                    gr.update(visible=True)
                )
            
            elif chunk.get('done'):
                # Final processing
                if chunk.get('sources') and not last_sources:
                    last_sources = chunk['sources']
                
                elapsed = time.time() - stream_start_ts
                metrics_line = f"â± {elapsed:.1f}s â€¢ æ€è€ƒ {len(thinking_text)} æ–‡å­— â€¢ å¿œç­” {len(answer_text)} æ–‡å­—"
                yield (
                    history,
                    gr.update(value="", visible=False),
                    gr.update(value=f"{metrics_line}\n\n" + thinking_text if thinking_text else ""),
                    gr.update(visible=show_thinking_enabled and is_thinking_model and bool(thinking_text)),
                    gr.update(visible=False)
                )
                break
    
    except Exception as e:
        error_message = f"âŒ ã‚¨ãƒ©ãƒ¼ â€¢ Error: {str(e)}"
        history.append({"role": "assistant", "content": error_message})
        yield (
            history,
            gr.update(value="", visible=False),
            gr.update(value=""),
            gr.update(visible=False),
            gr.update(visible=False)
        )

def enhanced_grammar_search(grammar_point, session_id):
    """Enhanced streaming grammar search"""
    global last_sources
    
    if not grammar_point.strip():
        yield (
            "æ–‡æ³•é …ç›®ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ â€¢ Please enter a grammar point to search for.", 
            "æº–å‚™å®Œäº† â€¢ Ready to search", 
            gr.update(visible=False)
        )
        return
    
    stop_event = threading.Event()
    session_stop_events[session_id] = stop_event
    
    try:
        yield "", "ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­... â€¢ Searching database...", gr.update(visible=True)
        
        # Use grammar-focused prompt
        original_prompt = assistant.prompt_template
        grammar_prompt = getattr(assistant, 'grammar_prompt_path', 'prompts/grammar_focused.md')
        
        if os.path.exists(grammar_prompt):
            assistant.prompt_template = assistant.load_prompt_template(grammar_prompt)
        
        full_response = ""
        
        # Stream with Japanese status updates
        for chunk in assistant.explain_grammar_stream(grammar_point, stop_event=stop_event):
            if stop_event.is_set():
                full_response += "\n\n*[ç”ŸæˆãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Generation stopped by user]*"
                yield full_response, "â¹ï¸ åœæ­¢ã•ã‚Œã¾ã—ãŸ â€¢ Stopped", gr.update(visible=False)
                break
            
            if chunk.get('token'):
                full_response += chunk['token']
                yield full_response, "ğŸ§  AIãƒ¢ãƒ‡ãƒ«ã§åˆ†æä¸­... â€¢ Analyzing with AI model...", gr.update(visible=True)
            
            if chunk.get('done'):
                # Update global sources for the sources viewer (same as chat)
                if chunk.get('sources'):
                    last_sources = chunk['sources']
                
                yield full_response, f"âœ… '{grammar_point}' ã®èª¬æ˜ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ â€¢ Found explanation for '{grammar_point}'", gr.update(visible=False)
        
        # Restore original prompt
        assistant.prompt_template = original_prompt
        
    except Exception as e:
        yield f"âŒ ã‚¨ãƒ©ãƒ¼ â€¢ Error: {str(e)}", "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ â€¢ Error occurred", gr.update(visible=False)
    finally:
        session_stop_events.pop(session_id, None)

def add_note_function(note_text, topic):
    """Enhanced note adding with bilingual feedback"""
    if not note_text.strip():
        return "ãƒãƒ¼ãƒˆå†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ â€¢ Please enter note content."
    
    try:
        vector_store.add_note(note_text, topic or "general")
        return f"âœ… ãƒãƒ¼ãƒˆãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸ â€¢ Note added successfully!\nãƒˆãƒ”ãƒƒã‚¯ â€¢ Topic: {topic or 'general'}"
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼ â€¢ Error adding note: {str(e)}"

def format_sources_markdown():
    """Enhanced sources formatting with bilingual labels"""
    global last_sources
    if not last_sources:
        return "ã¾ã å‡ºå…¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è³ªå•ã—ã¦å‡ºå…¸ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚\n\n*No sources yet. Ask a question to populate sources.*"
    
    lines = ["### ğŸ“š å‡ºå…¸æƒ…å ± â€¢ Source Information\n"]
    try:
        for i, s in enumerate(last_sources, 1):
            meta = s.get('metadata', {})
            src = meta.get('source', 'unknown')
            page = meta.get('page', 'N/A')
            snippet = (s.get('text') or '')[:150] + "..." if len(s.get('text', '')) > 150 else s.get('text', '')
            
            lines.append(f"**{i}. {src}** (ãƒšãƒ¼ã‚¸ â€¢ Page: {page})")
            lines.append(f"   *{snippet}*")
            lines.append("")
        
        return "\n".join(lines)
    except Exception as e:
        return f"å‡ºå…¸ã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ â€¢ Error displaying sources: {e}"

def process_new_document_enhanced(file, start_page=None, end_page=None, resume_from_json=True):
    """Enhanced document processing with bilingual status updates"""
    if not file:
        yield "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ â€¢ Please select a file."
        return
    
    try:
        yield f"ğŸ“„ å‡¦ç†é–‹å§‹ â€¢ Starting processing: {file.name}"
        
        # Check for existing JSON
        pdf_name = os.path.basename(file.name)
        json_path = os.path.join("processed_docs", f"{pdf_name}.json")
        
        if resume_from_json and os.path.exists(json_path):
            yield f"ğŸ“¥ æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­ â€¢ Loading from existing JSON: {json_path}"
            import json
            with open(json_path, 'r', encoding='utf-8') as f:
                ocr_data = json.load(f)
            yield f"âœ… JSONã‹ã‚‰ {len(ocr_data)} ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ â€¢ Loaded {len(ocr_data)} pages from JSON"
        else:
            # OCR processing with progress updates
            yield "ğŸ” OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­ â€¢ Extracting text with OCR..."
            ocr_data = []
            
            for page_data in ocr.process_pdf(file.name, start_page=start_page, end_page=end_page):
                if isinstance(page_data, str) and "Processing" in page_data:
                    # Status update
                    yield f"ğŸ“– {page_data}"
                else:
                    # Actual page data
                    ocr_data.append(page_data)
                    yield f"ğŸ“„ ãƒšãƒ¼ã‚¸å‡¦ç†å®Œäº† â€¢ Page processed: {len(ocr_data)} pages done"
        
        # Chunking
        yield f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–ä¸­ â€¢ Chunking text into segments..."
        chunks = vector_store.chunk_text(ocr_data)
        total_chunks = len(chunks)
        yield f"ğŸ“Š {total_chunks:,} ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ â€¢ Created {total_chunks:,} chunks"
        
        # Add to database
        if total_chunks > 1000:
            batch_size = 100
            total_batches = (total_chunks + batch_size - 1) // batch_size
            
            for i in range(0, total_chunks, batch_size):
                batch = chunks[i:i+batch_size]
                batch_num = i // batch_size + 1
                progress_pct = int((batch_num / total_batches) * 100)
                
                yield f"ğŸ’¾ ãƒãƒƒãƒè¿½åŠ ä¸­ â€¢ Adding batch {batch_num}/{total_batches} ({progress_pct}%): ãƒãƒ£ãƒ³ã‚¯ â€¢ chunks {i+1:,}-{min(i+batch_size, total_chunks):,}..."
                vector_store.add_documents(batch)
        else:
            yield f"ğŸ’¾ {total_chunks:,} ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ä¸­ â€¢ Adding {total_chunks:,} chunks to vector database..."
            vector_store.add_documents(chunks)
        
        # Final success message
        final_count = vector_store.collection.count()
        yield f"""âœ… å‡¦ç†å®Œäº† â€¢ Processing Complete!

ğŸ“Š è¿½åŠ ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•° â€¢ Chunks Added: {total_chunks:,}
ğŸ“š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç·æ–‡æ›¸æ•° â€¢ Total Documents in Database: {final_count:,}
ğŸ‰ å­¦ç¿’æº–å‚™å®Œäº† â€¢ Ready for learning!"""
        
    except Exception as e:
        yield f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ â€¢ Error during processing: {str(e)}"

# Create the enhanced Gradio interface
with gr.Blocks(
    title="ğŸŒ Classical Japanese Learning Assistant â€¢ å¤å…¸æ—¥æœ¬èªå­¦ç¿’ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
    theme=current_theme,
    css=CUSTOM_CSS + get_seasonal_css(current_season)
) as app:
    
    # Simple static header block
    gr.HTML(
        """
        <div class="simple-header">
            <h1 class="header-title">ğŸŒ Classical Japanese Learning Assistant</h1>
            <p class="header-subtitle">å¤å…¸æ—¥æœ¬èªå­¦ç¿’ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ â€¢ Learn Classical Japanese with AI and textbook knowledge</p>
        </div>
        """,
        elem_classes=["header-container"]
    )
    
    # Theme selector
    season_dropdown = create_seasonal_theme_selector()
    
    # Main navigation with enhanced structure
    with gr.Tabs(elem_classes=["tab-nav"]) as main_tabs:
        
        # Learning Hub - Main interface
        with gr.Tab("ğŸ“ å­¦ç¿’ â€¢ Learning Hub", elem_classes=["content-card"]):
            
            with gr.Tabs() as learning_subtabs:
                
                # Chat Interface
                with gr.Tab("ğŸ’¬ AIå¯¾è©± â€¢ AI Chat"):
                    chat_components = create_enhanced_chat_interface(
                        enhanced_chat_function, None, None, assistant
                    )
                    
                    # Wire up the chat functionality
                    outputs = [
                        chat_components['chatbot'],
                        chat_components['model_display'],
                        chat_components['thinking_content'],
                        chat_components['thinking_accordion'],
                        chat_components['stop_btn']
                    ]
                    
                    # Enhanced sources viewer
                    sources_components = create_enhanced_sources_viewer()

                    # Sentence Parser section (small dedicated input)
                    parser_components = create_sentence_parser_section()

                    def analyze_sentence(sentence):
                        """Analyze a Classical Japanese sentence using passage analysis prompt (with immediate feedback)"""
                        if not sentence or not sentence.strip():
                            yield "å…¥åŠ›ã•ã‚ŒãŸæ–‡ãŒã‚ã‚Šã¾ã›ã‚“ â€¢ Please enter a sentence."
                            return
                        if not assistant.model_name:
                            yield "âŒ ãƒ¢ãƒ‡ãƒ«æœªé¸æŠ â€¢ No model selected in Settings."
                            return
                        yield "ğŸ§  è§£æä¸­â€¦ â€¢ Analyzingâ€¦"
                        # Temporarily switch to passage analysis prompt
                        original_prompt = assistant.prompt_template
                        passage_prompt = 'prompts/passage_analysis.md'
                        try:
                            if os.path.exists(passage_prompt):
                                assistant.prompt_template = assistant.load_prompt_template(passage_prompt)
                            result = assistant.translate_passage(sentence)
                            yield result.get('answer', 'No analysis produced.')
                        except Exception as e:
                            yield f"âŒ è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ â€¢ Error during analysis: {e}"
                        finally:
                            assistant.prompt_template = original_prompt

                    parser_components['analyze_btn'].click(
                        analyze_sentence,
                        inputs=[parser_components['sentence_input']],
                        outputs=[parser_components['parser_output']],
                        show_progress="minimal"
                    )

                    # Dictionary lookup (click-to-lookup)
                    gr.Markdown("---")
                    gr.Markdown("### ğŸ“š èªå½™æ¤œç´¢ â€¢ Dictionary Lookup")
                    lookup_input = gr.Textbox(
                        label="èª â€¢ Term",
                        placeholder="èªã‚„è¡¨ç¾ã‚’å…¥åŠ› â€¢ Enter a term",
                        elem_classes=["enhanced-input"]
                    )
                    lookup_btn = gr.Button("æ¤œç´¢ â€¢ Lookup", variant="secondary")
                    lookup_out = gr.Markdown(elem_classes=["explanation-card"]) 

                    def lookup_term(term):
                        import unicodedata
                        term = (term or "").strip()
                        if not term:
                            return "å…¥åŠ›ãŒã‚ã‚Šã¾ã›ã‚“ â€¢ Please enter a term."
                        if not dictionary_entries:
                            return "è¾æ›¸ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ â€¢ No dictionary loaded in Settings."
                        q = unicodedata.normalize('NFKC', term)
                        results = []
                        for e in dictionary_entries:
                            hw = e.get('headword') or e.get('term') or e.get('word') or ''
                            rd = e.get('reading') or e.get('yomi') or ''
                            gl = e.get('gloss') or e.get('definition') or e.get('def') or e.get('meaning') or ''
                            if any(q in unicodedata.normalize('NFKC', str(x)) for x in (hw, rd, gl)):
                                results.append((hw, rd, gl, e))
                            if len(results) >= 20:
                                break
                        if not results:
                            return f"è©²å½“ãªã— â€¢ No matches for: {term}"
                        lines = [f"**ğŸ” {term} â€” {len(results)} ä»¶ â€¢ matches**\n"]
                        for i, (hw, rd, gl, e) in enumerate(results, 1):
                            pos = e.get('pos') or e.get('å“è©') or ''
                            src = e.get('source') or ''
                            line = f"**{i}. {hw}** "
                            if rd:
                                line += f"({rd}) "
                            if pos:
                                line += f"[{pos}] "
                            line += "\n- " + str(gl)
                            if src:
                                line += "\n  _" + str(src) + "_"
                            lines.append(line)
                        return "\n\n".join(lines)

                    lookup_btn.click(lookup_term, inputs=[lookup_input], outputs=[lookup_out], show_progress="minimal")
                    
                    # Chat event handlers
                    submit_event = chat_components['msg'].submit(
                        enhanced_chat_function,
                        [
                            chat_components['msg'],
                            chat_components['chatbot'],
                            chat_components['show_thinking'],
                            chat_components['knowledge_mode'],
                            chat_components['session_id_state']
                        ],
                        outputs,
                        show_progress="minimal"
                    ).then(
                        lambda: gr.update(visible=False), None, chat_components['stop_btn']
                    ).then(
                        lambda: "", None, chat_components['msg']
                    )
                    
                    click_event = chat_components['submit_btn'].click(
                        enhanced_chat_function,
                        [
                            chat_components['msg'],
                            chat_components['chatbot'],
                            chat_components['show_thinking'],
                            chat_components['knowledge_mode'],
                            chat_components['session_id_state']
                        ],
                        outputs,
                        show_progress="minimal"
                    ).then(
                        lambda: gr.update(visible=False), None, chat_components['stop_btn']
                    ).then(
                        lambda: "", None, chat_components['msg']
                    )
                    
                    # Show stop button during generation
                    chat_components['msg'].submit(lambda: gr.update(visible=True), None, chat_components['stop_btn'], queue=False)
                    chat_components['submit_btn'].click(lambda: gr.update(visible=True), None, chat_components['stop_btn'], queue=False)
                    
                    # Stop button functionality
                    def stop_generation_handler(current_session_id):
                        stop_event = session_stop_events.get(current_session_id)
                        if stop_event:
                            stop_event.set()
                        return gr.update(visible=False)
                    
                    chat_components['stop_btn'].click(
                        stop_generation_handler,
                        inputs=[chat_components['session_id_state']],
                        outputs=[chat_components['stop_btn']],
                        cancels=[submit_event, click_event],
                        queue=False
                    )
                    
                    # Clear functionality
                    def clear_all():
                        return (
                            [],
                            gr.update(value="", visible=False),
                            gr.update(value=""),
                            gr.update(visible=False),
                            gr.update()
                        )
                    
                    chat_components['clear_btn'].click(clear_all, None, outputs, queue=False)
                    
                    # Sources refresh
                    sources_components['refresh_sources_btn'].click(
                        format_sources_markdown, None, sources_components['sources_md']
                    )
                
                # Grammar Search
                with gr.Tab("ğŸ“– æ–‡æ³•æ¤œç´¢ â€¢ Grammar Search"):
                    grammar_components = create_enhanced_grammar_search(enhanced_grammar_search)
                    
                    # Add sources viewer to grammar tab (same as chat)
                    grammar_sources_components = create_enhanced_sources_viewer()
                    
                    # Wire up grammar search
                    search_event = grammar_components['search_btn'].click(
                        enhanced_grammar_search,
                        inputs=[grammar_components['grammar_input'], grammar_components['grammar_session_id']],
                        outputs=[
                            grammar_components['grammar_output'],
                            grammar_components['grammar_status'],
                            grammar_components['stop_grammar_btn']
                        ],
                        show_progress="minimal"
                    )
                    
                    # Sources refresh for grammar tab
                    grammar_sources_components['refresh_sources_btn'].click(
                        format_sources_markdown, None, grammar_sources_components['sources_md']
                    )
                    
                    # Grammar stop button functionality
                    def stop_grammar_generation(session_id):
                        """Stop grammar generation for this session"""
                        if session_id in session_stop_events:
                            session_stop_events[session_id].set()
                        return gr.update(visible=False)
                    
                    grammar_components['stop_grammar_btn'].click(
                        stop_grammar_generation,
                        inputs=[grammar_components['grammar_session_id']],
                        outputs=[grammar_components['stop_grammar_btn']],
                        cancels=[search_event],
                        queue=False
                    )
                    
                    # Show stop button during grammar search
                    grammar_components['search_btn'].click(
                        lambda: gr.update(visible=True), 
                        None, 
                        grammar_components['stop_grammar_btn'], 
                        queue=False
                    )
                    
                    # Example button handlers
                    for i, btn in enumerate(grammar_components['example_buttons']):
                        example_text = ["ã‚‰ã‚€", "ã¹ã—", "ãªã‚Š", "ã‘ã‚Š", "ã¤ãƒ»ã¬", "ã‚€ãƒ»ã¹ã—"][i]
                        btn.click(
                            lambda x=example_text: x,
                            None,
                            grammar_components['grammar_input'],
                            queue=False
                        )
                
                # Study Notes
                with gr.Tab("ğŸ“ å­¦ç¿’ãƒãƒ¼ãƒˆ â€¢ Study Notes"):
                    notes_components = create_notes_interface(add_note_function)
                    
                    notes_components['add_btn'].click(
                        add_note_function,
                        [notes_components['note_input'], notes_components['topic_input']],
                        notes_components['note_output']
                    )
        
        # Document Management
        with gr.Tab("ğŸ“š æ–‡æ›¸ç®¡ç† â€¢ Document Management"):
            
            with gr.Tabs():
                
                # Add Documents
                with gr.Tab("ğŸ“¥ æ–‡æ›¸è¿½åŠ  â€¢ Add Documents"):
                    gr.Markdown(
                        """
                        ### ğŸ“„ æ–°ã—ã„æ–‡æ›¸ã‚’è¿½åŠ  â€¢ Add New Documents
                        Upload PDF files or images to expand your knowledge base.
                        """
                    )
                    
                    file_input = gr.File(
                        label="ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ â€¢ Select File",
                        file_types=[".pdf", ".jpg", ".png"],
                        elem_classes=["enhanced-file-input"]
                    )
                    
                    with gr.Row():
                        start_page_in = gr.Number(
                            label="é–‹å§‹ãƒšãƒ¼ã‚¸ â€¢ Start Page",
                            value=None,
                            elem_classes=["enhanced-input"]
                        )
                        end_page_in = gr.Number(
                            label="çµ‚äº†ãƒšãƒ¼ã‚¸ â€¢ End Page",
                            value=None,
                            elem_classes=["enhanced-input"]
                        )
                    
                    resume_chk = gr.Checkbox(
                        label="æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ â€¢ Import from existing JSON if present",
                        value=True,
                        info="JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€OCRã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èª­ã¿è¾¼ã¿ã¾ã™ â€¢ If matching JSON exists, skip OCR and import it",
                        elem_classes=["japanese-checkbox"]
                    )
                    
                    process_btn = gr.Button(
                        "ğŸš€ æ–‡æ›¸å‡¦ç†é–‹å§‹ â€¢ Start Processing",
                        variant="primary",
                        elem_classes=["btn-primary", "process-button"]
                    )
                    
                    process_output = gr.Textbox(
                        label="å‡¦ç†çŠ¶æ³ â€¢ Processing Status",
                        lines=10,
                        elem_classes=["status-display", "process-log"]
                    )
                    
                    process_btn.click(
                        process_new_document_enhanced,
                        [file_input, start_page_in, end_page_in, resume_chk],
                        process_output
                    )
                
                # Database Management
                with gr.Tab("ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç† â€¢ Database Management"):
                    gr.Markdown(
                        """
                        ### ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ â€¢ Database Statistics
                        View and manage your textbook database.
                        """
                    )
                    
                    # Database statistics
                    db_stats_display = gr.Markdown("Click 'Refresh Stats' to view database information")
                    png_stats_display = gr.Markdown("PNG file information will appear here")
                    refresh_db_stats_btn = gr.Button(
                        "ğŸ”„ çµ±è¨ˆæ›´æ–° â€¢ Refresh Stats",
                        variant="primary",
                        elem_classes=["btn-primary"]
                    )
                    
                    gr.Markdown("---")
                    
                    # Delete textbook section
                    gr.Markdown("### âš ï¸ æ–‡æ›¸å‰Šé™¤ â€¢ Delete Documents")
                    gr.Markdown("**è­¦å‘Š â€¢ Warning**: é¸æŠã—ãŸæ–‡æ›¸ã®ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãŒæ°¸ç¶šçš„ã«å‰Šé™¤ã•ã‚Œã¾ã™ â€¢ This permanently removes all chunks from the selected textbook")
                    
                    textbook_dropdown = gr.Dropdown(
                        choices=[],
                        label="å‰Šé™¤ã™ã‚‹æ–‡æ›¸ã‚’é¸æŠ â€¢ Select Document to Delete",
                        info="å®Œå…¨ã«å‰Šé™¤ã™ã‚‹æ–‡æ›¸ã‚’é¸æŠã—ã¦ãã ã•ã„ â€¢ Choose which document to remove completely",
                        elem_classes=["enhanced-dropdown"]
                    )
                    
                    confirm_input = gr.Textbox(
                        label="å‰Šé™¤ç¢ºèª â€¢ Confirmation",
                        placeholder="Type 'DELETE filename.pdf' to confirm",
                        info="æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ â€¢ Type exactly: DELETE [document filename]",
                        elem_classes=["enhanced-input"]
                    )
                    
                    delete_btn = gr.Button(
                        "ğŸ—‘ï¸ æ–‡æ›¸å‰Šé™¤ â€¢ Delete Document",
                        variant="stop",
                        elem_classes=["btn-secondary"]
                    )
                    
                    delete_status = gr.Textbox(
                        label="å‰Šé™¤çŠ¶æ³ â€¢ Deletion Status",
                        interactive=False,
                        elem_classes=["status-display"]
                    )
                    
                    # Database management functions
                    def get_database_stats():
                        """Get current database statistics"""
                        try:
                            stats = db_manager.get_textbook_stats()
                            if 'error' in stats:
                                return f"âŒ Error: {stats['error']}", [], ""
                            
                            # Get PNG statistics
                            png_info = db_manager.get_png_stats()
                            png_summary = ""
                            if png_info['count'] > 0:
                                size_display = f"{png_info['size_gb']} GB" if png_info['size_gb'] >= 1 else f"{png_info['size_mb']} MB"
                                png_summary = f"ğŸ“ **PNG Files**: {png_info['count']} files ({size_display} total)"
                            else:
                                png_summary = "ğŸ“ **PNG Files**: No PNG files found"
                            
                            # Format textbook list
                            textbook_list = []
                            for source, count in stats['textbooks'].items():
                                textbook_list.append(f"ğŸ“š **{source}**: {count:,} chunks")
                            
                            textbook_info = "\\n".join(textbook_list)
                            
                            summary = f"""## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¦‚è¦ â€¢ Database Overview
**ç·æ–‡æ›¸æ•° â€¢ Total Documents**: {stats['total_documents']:,} chunks
**æ–‡æ›¸æ•° â€¢ Number of Books**: {len(stats['textbooks'])} documents
**é‡è¤‡ â€¢ Duplicates Found**: {stats['duplicates']:,} entries
{png_summary}

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®æ–‡æ›¸ â€¢ Documents in Database:
{textbook_info}
                            """
                            
                            # Return dropdown options for deletion
                            textbook_options = list(stats['textbooks'].keys())
                            return summary, textbook_options, png_summary
                        except Exception as e:
                            error_msg = f"âŒ Error getting stats: {str(e)}"
                            return error_msg, [], error_msg
                    
                    def delete_textbook_func(textbook_name, confirm_text):
                        """Delete a textbook from the database"""
                        if not textbook_name:
                            return "æ–‡æ›¸ã‚’é¸æŠã—ã¦ãã ã•ã„ â€¢ Please select a document to delete."
                        
                        if confirm_text != f"DELETE {textbook_name}":
                            return f"âŒ æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ â€¢ Please type exactly: DELETE {textbook_name}"
                        
                        try:
                            result = db_manager.delete_textbook(textbook_name)
                            if result['success']:
                                return f"âœ… {result['message']}"
                            else:
                                return f"âŒ {result['message']}"
                        except Exception as e:
                            return f"âŒ Error deleting document: {str(e)}"
                    
                    # Connect database management functions
                    def refresh_stats():
                        stats_text, textbook_options, png_info = get_database_stats()
                        return stats_text, gr.update(choices=textbook_options), png_info
                    
                    refresh_db_stats_btn.click(
                        refresh_stats,
                        outputs=[db_stats_display, textbook_dropdown, png_stats_display]
                    )
                    
                    delete_btn.click(
                        delete_textbook_func,
                        inputs=[textbook_dropdown, confirm_input],
                        outputs=[delete_status]
                    )
                    
                    gr.Markdown("---")
                    
                    # PNG cleanup section
                    gr.Markdown("### ğŸ“ PNG ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ â€¢ Clean PNG Files")
                    gr.Markdown("**âš ï¸ è­¦å‘Š â€¢ Warning**: PNGãƒ•ã‚¡ã‚¤ãƒ«ã¯OCRå‡¦ç†ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚")
                    gr.Markdown("**âš ï¸ Warning**: PNG files are used for OCR processing. Delete only after confirming database is working correctly.")
                    gr.Markdown("ã“ã‚Œã«ã‚ˆã‚Šã€processed_docsãƒ•ã‚©ãƒ«ãƒ€ãƒ¼å†…ã®ã™ã¹ã¦ã®PNGãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯å¾©æ—§ã®ãŸã‚ã«ä¿æŒã•ã‚Œã¾ã™ã€‚")
                    gr.Markdown("This will delete ALL PNG files in processed_docs folder. JSON files will be preserved for recovery.")
                    
                    # PNG stats are shown above with the main database stats
                    
                    png_confirm_input = gr.Textbox(
                        label="å‰Šé™¤ç¢ºèª â€¢ Confirmation Required",
                        placeholder="Type 'DELETE PNGs' to confirm",
                        info="æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ â€¢ Type exactly: DELETE PNGs",
                        elem_classes=["enhanced-input"]
                    )
                    
                    delete_pngs_btn = gr.Button(
                        "ğŸ—‘ï¸ PNG ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ â€¢ Delete PNG Files",
                        variant="stop",
                        elem_classes=["btn-secondary"]
                    )
                    
                    png_delete_status = gr.Textbox(
                        label="PNGå‰Šé™¤çŠ¶æ³ â€¢ PNG Deletion Status",
                        interactive=False,
                        elem_classes=["status-display"]
                    )
                    
                    def delete_png_files_func(confirm_text):
                        """Delete PNG files after confirmation"""
                        if confirm_text != "DELETE PNGs":
                            return "âŒ 'DELETE PNGs' ã¨æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ â€¢ Please type 'DELETE PNGs' exactly to confirm deletion."
                        
                        try:
                            result = db_manager.delete_png_files()
                            if result['success']:
                                return f"âœ… {result['message']}"
                            else:
                                return f"âŒ {result['message']}"
                        except Exception as e:
                            return f"âŒ Error deleting PNG files: {str(e)}"
                    
                    delete_pngs_btn.click(
                        delete_png_files_func,
                        inputs=[png_confirm_input],
                        outputs=[png_delete_status]
                    )

                    gr.Markdown("---")
                    # Orphaned JSON Import
                    gr.Markdown("### ğŸ“¥ JSON ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â€¢ Import Orphaned JSON")
                    gr.Markdown("processed_docs å†…ã®JSONã§ã€ã¾ã ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å–ã‚Šè¾¼ã¾ã‚Œã¦ã„ãªã„ã‚‚ã®ã‚’æ¤œå‡ºã—ã¦å–ã‚Šè¾¼ã¿ã¾ã™ â€¢ Scan and import processed JSON files not yet in the database.")
                    scan_json_btn = gr.Button("ğŸ” JSONã‚¹ã‚­ãƒ£ãƒ³ â€¢ Scan for JSON Files", variant="secondary")
                    json_list = gr.CheckboxGroup(label="æ¤œå‡ºã•ã‚ŒãŸJSON â€¢ Found JSON Files (select to import)")
                    import_selected_btn = gr.Button("ğŸ“¥ é¸æŠã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â€¢ Import Selected", variant="primary")
                    import_all_btn = gr.Button("ğŸ“¥ ã™ã¹ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â€¢ Import All", variant="secondary")
                    import_status = gr.Markdown("")

                    def scan_orphaned_json():
                        import glob, json
                        files = sorted(glob.glob('processed_docs/*.json'))
                        orphaned = []
                        for f in files:
                            name = os.path.basename(f)
                            source_key = os.path.splitext(name)[0]
                            try:
                                with open(f, 'r', encoding='utf-8') as jf:
                                    data = json.load(jf)
                                if isinstance(data, list) and data:
                                    sp = data[0].get('source_pdf') or data[0].get('metadata', {}).get('source')
                                    if sp:
                                        source_key = sp
                            except Exception:
                                pass
                            try:
                                count = vector_store.collection.count(where={"source": source_key})
                            except Exception:
                                count = 0
                            if count == 0:
                                orphaned.append(name)
                        return gr.update(choices=orphaned, value=[])

                    def import_json_files(selected):
                        import json
                        if not selected:
                            return "âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ â€¢ No files selected"
                        total_added = 0
                        for name in selected:
                            path = os.path.join('processed_docs', name)
                            try:
                                with open(path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                chunks = vector_store.chunk_text(data)
                                vector_store.add_documents(chunks)
                                total_added += len(chunks)
                            except Exception as e:
                                return f"âŒ {name} ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•— â€¢ Failed on {name}: {e}"
                        return f"âœ… {len(selected)} ä»¶ã®JSONã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â€¢ Imported {len(selected)} JSON files, è¿½åŠ  â€¢ added ~{total_added:,} ãƒãƒ£ãƒ³ã‚¯ â€¢ chunks"

                    scan_json_btn.click(scan_orphaned_json, None, json_list)
                    import_selected_btn.click(import_json_files, json_list, import_status)
                    import_all_btn.click(lambda choices: choices, json_list, json_list).then(import_json_files, json_list, import_status)
        
        # System & Settings
        with gr.Tab("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ  â€¢ System"):
            
            # Dashboard
            dashboard_components = create_dashboard_interface(None)
            
            # Connect dashboard functions
            def update_dashboard_stats():
                """Update dashboard statistics"""
                try:
                    # Get detailed database statistics
                    stats = db_manager.get_textbook_stats()
                    if 'error' in stats:
                        error_msg = f"**âŒ Error**\n\n{stats['error']}"
                        return error_msg, error_msg, error_msg
                    
                    # Enhanced document display
                    textbook_list = []
                    for source, count in stats['textbooks'].items():
                        textbook_list.append(f"â€¢ **{source}**: {count:,} chunks")
                    
                    if textbook_list:
                        textbook_info = "\\n".join(textbook_list)
                        docs_display = f"""**ğŸ“š ç·æ–‡æ›¸æ•° â€¢ Total Documents**

**{stats['total_documents']:,}** total chunks
**{len(stats['textbooks'])}** documents

### æ–‡æ›¸ä¸€è¦§ â€¢ Document Library:
{textbook_info}"""
                    else:
                        docs_display = "**ğŸ“š ç·æ–‡æ›¸æ•° â€¢ Total Documents**\n\nNo documents in database"
                    
                    # Routing statistics
                    routing_display = get_routing_stats_display()
                    
                    # Grammar points placeholder  
                    grammar_display = "**ğŸ“– æ–‡æ³•é …ç›® â€¢ Grammar Points**\n\nStudied: 0\nMastered: 0"
                    
                    return docs_display, routing_display, grammar_display
                except Exception as e:
                    error_msg = f"**âŒ Error**\n\n{str(e)}"
                    return error_msg, error_msg, error_msg
            
            def run_health_checks():
                """Run comprehensive health checks"""
                messages = []
                
                # Ollama check
                try:
                    import subprocess
                    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
                    if result.returncode == 0:
                        models = [line.split()[0] for line in result.stdout.strip().split('\n')[1:] if line.strip()]
                        messages.append(f"âœ… Ollama reachable. Models: {', '.join(models) if models else 'none'}")
                    else:
                        messages.append("âŒ Ollama not responding to 'ollama list'")
                except Exception as e:
                    messages.append(f"âŒ Ollama check failed: {e}")
                
                # Tesseract check
                try:
                    import subprocess
                    result = subprocess.run(['tesseract', '--list-langs'], capture_output=True, text=True)
                    has_jpn = 'jpn' in result.stdout
                    messages.append("âœ… Tesseract installed" + (" with 'jpn'" if has_jpn else " (missing 'jpn' language)"))
                except Exception as e:
                    messages.append(f"âŒ Tesseract check failed: {e}")
                
                # ChromaDB check
                try:
                    count = vector_store.collection.count()
                    messages.append(f"âœ… ChromaDB OK. Documents: {count}")
                except Exception as e:
                    messages.append(f"âŒ ChromaDB check failed: {e}")
                
                return "\\n\\n".join(messages)
            
            def get_routing_stats_display():
                """Get hybrid knowledge system routing statistics"""
                try:
                    stats = assistant.get_routing_stats()
                    if stats['total'] == 0:
                        return "ğŸ“Š **Knowledge Routing Statistics**\\n\\nNo queries processed yet."
                    
                    lines = [
                        f"ğŸ“Š **Knowledge Routing Statistics**\\n",
                        f"**Total Queries**: {stats['total']}",
                        f"**Average Confidence**: {stats['avg_confidence']:.1%}\\n",
                        "**Route Distribution**:"
                    ]
                    
                    route_emojis = {
                        'RAG': 'ğŸ“š',
                        'GENERAL': 'ğŸ§ ', 
                        'HYBRID': 'ğŸ”„'
                    }
                    
                    for route, percentage in stats['route_percentages'].items():
                        emoji = route_emojis.get(route, 'â“')
                        lines.append(f"- {emoji} **{route}**: {percentage:.1f}%")
                    
                    return "\\n".join(lines)
                
                except Exception as e:
                    return f"âŒ Routing stats error: {e}"
            
            def create_backup():
                """Create database backup"""
                try:
                    import shutil
                    import datetime
                    import os
                    
                    os.makedirs('backups', exist_ok=True)
                    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
                    base_name = os.path.join('backups', f'chroma_db-{timestamp}')
                    archive_path = shutil.make_archive(base_name, 'zip', root_dir='.', base_dir='chroma_db')
                    
                    return f"âœ… Backup created: {archive_path}"
                except Exception as e:
                    return f"âŒ Backup failed: {str(e)}"
            
            # Connect dashboard buttons
            dashboard_components['refresh_stats_btn'].click(
                update_dashboard_stats,
                None,
                [
                    dashboard_components['total_docs_display'],
                    dashboard_components['study_time_display'], 
                    dashboard_components['grammar_points_display']
                ]
            )
            
            dashboard_components['health_check_btn'].click(
                lambda: gr.update(value=run_health_checks()),
                None,
                dashboard_components['total_docs_display']  # Reuse for health check display
            )
            
            dashboard_components['backup_btn'].click(
                lambda: gr.update(value=create_backup()),
                None,
                dashboard_components['study_time_display']  # Reuse for backup status
            )
            
            gr.Markdown("---")
            
            # Model Settings
            gr.Markdown("### ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š â€¢ Model Settings")
            
            def get_installed_models():
                """Get list of installed Ollama models"""
                try:
                    import subprocess
                    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
                    if result.returncode == 0:
                        lines = result.stdout.strip().split('\n')
                        if len(lines) > 1:
                            models = []
                            for line in lines[1:]:
                                if line.strip():
                                    parts = line.strip().split()
                                    if parts:
                                        models.append(parts[0])
                            return models if models else []
                    return []
                except Exception as e:
                    logging.getLogger(__name__).warning(f"Error getting models: {e}")
                    return []
            
            installed_models = get_installed_models()
            current_model = assistant.model_name if assistant.model_name in installed_models else (installed_models[0] if installed_models else None)
            
            model_dropdown = gr.Dropdown(
                choices=installed_models,
                value=current_model,
                label="ãƒ¢ãƒ‡ãƒ«é¸æŠ â€¢ Select Model",
                info="ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®Ollamaãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é¸æŠ â€¢ Choose from installed Ollama models",
                elem_classes=["enhanced-dropdown"]
            )

            gr.Markdown("---")
            # Dictionary Loader
            gr.Markdown("### ğŸ“š è¾æ›¸è¨­å®š â€¢ Dictionary Settings")
            gr.Markdown("ãƒ­ãƒ¼ã‚«ãƒ«è¾æ›¸(JSON)ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒƒãƒˆã§èªå½™æ¤œç´¢ãŒã§ãã¾ã™ â€¢ Load a local JSON dictionary for lookups in Chat.")
            dict_file = gr.File(label="è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«(JSON) â€¢ Dictionary JSON", file_types=[".json"])
            load_dict_btn = gr.Button("ğŸ“¥ è¾æ›¸èª­ã¿è¾¼ã¿ â€¢ Load Dictionary", variant="secondary")
            dict_status = gr.Markdown("")

            def load_dictionary(file_obj):
                import json, unicodedata
                global dictionary_entries
                if not file_obj:
                    return "âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ â€¢ No file selected"
                try:
                    with open(file_obj.name, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    entries = []
                    if isinstance(data, dict):
                        # Map form: headword -> definition
                        for k, v in data.items():
                            entries.append({
                                'headword': unicodedata.normalize('NFKC', str(k)),
                                'gloss': unicodedata.normalize('NFKC', str(v))
                            })
                    elif isinstance(data, list):
                        for it in data:
                            if isinstance(it, dict):
                                # Normalize known fields
                                it = {k: (unicodedata.normalize('NFKC', str(v)) if isinstance(v, str) else v) for k, v in it.items()}
                                entries.append(it)
                    else:
                        return "âŒ æœªå¯¾å¿œã®JSONå½¢å¼ â€¢ Unsupported JSON structure"
                    dictionary_entries = entries
                    return f"âœ… è¾æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ â€¢ Loaded dictionary with {len(entries):,} entries"
                except Exception as e:
                    return f"âŒ è¾æ›¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ â€¢ Failed to load dictionary: {e}"

            load_dict_btn.click(load_dictionary, inputs=[dict_file], outputs=[dict_status], show_progress="minimal")
            
            def switch_model(model_name):
                assistant.model_name = model_name
                return f"ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ â€¢ Switched to model: {model_name}"
            
            model_status = gr.Textbox(
                label="ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ â€¢ Model Status",
                value=f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ« â€¢ Current: {assistant.model_name}",
                interactive=False,
                elem_classes=["status-display"]
            )
            
            model_dropdown.change(switch_model, model_dropdown, model_status)
            
            refresh_models_btn = gr.Button(
                "ğŸ”„ ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆæ›´æ–° â€¢ Refresh Model List",
                elem_classes=["btn-secondary"]
            )
            
            def refresh_models():
                models = get_installed_models()
                if not models:
                    return gr.update(choices=[], value=None), "ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ â€¢ No models found. Please install Ollama models."
                
                current_value = assistant.model_name if assistant.model_name in models else models[0]
                return (
                    gr.update(choices=models, value=current_value),
                    f"{len(models)} ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ â€¢ Found {len(models)} installed models: {', '.join(models)}"
                )
            
            refresh_models_btn.click(refresh_models, None, [model_dropdown, model_status])

# Launch the enhanced app
if __name__ == "__main__":
    # Enable queuing for streaming
    app.queue()
    # Launch with enhanced configuration
    app.launch(
        server_name=settings.gradio_host,
        server_port=settings.gradio_port,
        share=False,
        show_api=False,
        show_error=True
    )
